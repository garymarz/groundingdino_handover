{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入模型\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/checkpoint.pth\") # swin_T\n",
    "# model = load_model(\"groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"weights/groundingdino_swinb_cogcoor.pth\") # swin_B\n",
    "\n",
    "# 圖片時若有lora要改成True\n",
    "use_lora = False\n",
    "lora_checkpoint = 'D:\\\\GroundingDINO_live\\\\Open-GroundingDino\\\\output\\\\checkpoint0017_lora_prompt.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 套件\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class LoRA_qkv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            qkv,\n",
    "            linear_a_q: nn.Module,\n",
    "            linear_b_q: nn.Module,\n",
    "            linear_a_v: nn.Module,\n",
    "            linear_b_v: nn.Module,):\n",
    "        super().__init__()\n",
    "        self.qkv = qkv\n",
    "        self.linear_a_q = linear_a_q\n",
    "        self.linear_b_q = linear_b_q\n",
    "        self.linear_a_v = linear_a_v\n",
    "        self.linear_b_v = linear_b_v\n",
    "        self.d_model = qkv.in_features\n",
    "        self.w_identity = torch.eye(qkv.in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv(x)\n",
    "        q_ba = self.linear_b_q(self.linear_a_q(x))\n",
    "        v_ba = self.linear_b_v(self.linear_a_v(x))\n",
    "        qkv[:, :,  :self.d_model] += q_ba # q part\n",
    "        qkv[:, :,  -self.d_model:] += v_ba # v part\n",
    "        return qkv\n",
    "    \n",
    "class LoRA_gdswin(nn.Module):\n",
    "    def __init__(self, model, rank=256):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        assert rank > 0\n",
    "        # base_vit_dim = sam_model.image_encoder.patch_embed.proj.out_channels\n",
    "        self.A_weights = []\n",
    "        self.B_weights = []\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for layer in model.backbone[0].layers:\n",
    "            for blk in layer.blocks:\n",
    "                w_qkv_linear = blk.attn.qkv\n",
    "                self.d_model = w_qkv_linear.in_features\n",
    "                w_a_linear_q = nn.Linear(self.d_model, self.rank, bias=False)\n",
    "                w_b_linear_q = nn.Linear(self.rank, self.d_model, bias=False)\n",
    "                w_a_linear_v = nn.Linear(self.d_model, self.rank, bias=False)\n",
    "                w_b_linear_v = nn.Linear(self.rank, self.d_model, bias=False)\n",
    "                self.A_weights.append(w_a_linear_q)\n",
    "                self.B_weights.append(w_b_linear_q)\n",
    "                self.A_weights.append(w_a_linear_v)\n",
    "                self.B_weights.append(w_b_linear_v)\n",
    "                blk.attn.qkv = LoRA_qkv(\n",
    "                    w_qkv_linear,\n",
    "                    w_a_linear_q,\n",
    "                    w_b_linear_q,\n",
    "                    w_a_linear_v,\n",
    "                    w_b_linear_v\n",
    "                )\n",
    "        self.reset_parameters()\n",
    "        self.lora_model = model\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initalisation like in the paper\n",
    "        for w_A in self.A_weights:\n",
    "            nn.init.kaiming_uniform_(w_A.weight, a=np.sqrt(5))\n",
    "        for w_B in self.B_weights:\n",
    "            nn.init.zeros_(w_B.weight)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, rank):\n",
    "        super().__init__()\n",
    "        self.original_linear = original_linear\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Linear(original_linear.in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, original_linear.out_features, bias=False)\n",
    "        nn.init.normal_(self.lora_A.weight, std=0.02)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.original_linear(x) + self.lora_B(self.lora_A(x))\n",
    "\n",
    "\n",
    "if use_lora:\n",
    "    model = LoRA_gdswin(model, 512)\n",
    "    model = model.lora_model\n",
    "    model.load_state_dict(torch.load(lora_checkpoint)['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試單張圖片\n",
    "BOX_TRESHOLD = 0.2\n",
    "TEXT_TRESHOLD = 0.2\n",
    "IMAGE_PATH = r'C:\\Users\\yangu\\Desktop\\testt\\AA90TL43J2PAT152053601-01.jpg' # 圖片位置\n",
    "TEXT_PROMPT = 'the right defect' # prompt\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測用function\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import supervision as sv\n",
    "\n",
    "def create_xml(objects, filename_text, folder_text, width_text, height_text, save_path):\n",
    "    annotation = ET.Element(\"annotation\")\n",
    "    # 添加子節點\n",
    "    folder = ET.SubElement(annotation, \"folder\")\n",
    "    folder.text = folder_text\n",
    "    filename = ET.SubElement(annotation, \"filename\")\n",
    "    filename.text = filename_text\n",
    "    size = ET.SubElement(annotation, \"size\")\n",
    "    width = ET.SubElement(size, \"width\")\n",
    "    width.text = width_text\n",
    "    height = ET.SubElement(size, \"height\")\n",
    "    height.text = height_text\n",
    "    depth = ET.SubElement(size, \"depth\")\n",
    "    depth.text = \"3\"\n",
    "    segmented = ET.SubElement(annotation, \"segmented\")\n",
    "    segmented.text = \"0\"\n",
    "    for obj in objects:\n",
    "        object_node = ET.SubElement(annotation, \"object\")\n",
    "        name = ET.SubElement(object_node, \"name\")\n",
    "        name.text = obj[\"name\"]\n",
    "        truncated = ET.SubElement(object_node, \"truncated\")\n",
    "        truncated.text = \"0\"\n",
    "        difficult = ET.SubElement(object_node, \"difficult\")\n",
    "        difficult.text = \"0\"\n",
    "        bndbox = ET.SubElement(object_node, \"bndbox\")\n",
    "        xmin = ET.SubElement(bndbox, \"xmin\")\n",
    "        xmin.text = obj[\"xmin\"]\n",
    "        xmax = ET.SubElement(bndbox, \"xmax\")\n",
    "        xmax.text = obj[\"xmax\"]\n",
    "        ymin = ET.SubElement(bndbox, \"ymin\")\n",
    "        ymin.text = obj[\"ymin\"]\n",
    "        ymax = ET.SubElement(bndbox, \"ymax\")\n",
    "        ymax.text = obj[\"ymax\"]\n",
    "        \n",
    "    # 創建 ElementTree 物件\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    # 將樹寫入文件\n",
    "    tree.write(os.path.join(save_path, filename_text[:-4]+'.xml'))\n",
    "\n",
    "def bbox_convert(boxes,image_source):\n",
    "    '''\n",
    "    Filter background with opencv match template, \n",
    "    '''\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    return xyxy\n",
    "\n",
    "\n",
    "def non_maximum_suppression_fast(boxes, overlapThresh=0.3):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    pick = []\n",
    "    x1 = boxes[:,0].astype(\"float\")\n",
    "    y1 = boxes[:,1].astype(\"float\")\n",
    "    x2 = boxes[:,2].astype(\"float\")\n",
    "    y2 = boxes[:,3].astype(\"float\")\n",
    "    bound_area = (x2-x1+1) * (y2-y1+1)\n",
    "    sort_index = np.argsort(y2)\n",
    "    while sort_index.shape[0] > 0:\n",
    "        last = sort_index.shape[0]-1\n",
    "        i = sort_index[last]\n",
    "        pick.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[sort_index[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[sort_index[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[sort_index[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[sort_index[:last]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        overlap = (w*h) / bound_area[sort_index[:last]]\n",
    "        sort_index = np.delete(sort_index, np.concatenate(([last], np.where(overlap > overlapThresh)[0]))) \n",
    "    return boxes[pick]\n",
    "\n",
    "def nms(bounding_boxes, confidence_score, threshold=0.4):\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return [], []\n",
    "    boxes = np.array(bounding_boxes)\n",
    "    start_x = boxes[:, 0]\n",
    "    start_y = boxes[:, 1]\n",
    "    end_x = boxes[:, 2]\n",
    "    end_y = boxes[:, 3]\n",
    "    # Confidence scores of bounding boxes\n",
    "    confidence_score = confidence_score.numpy()\n",
    "    score = confidence_score\n",
    "    # Piced bounding boxes\n",
    "    picked_boxes = []\n",
    "    picked_score = []\n",
    "    # Compute areas of bounding boxes\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "    # Sort by confidence score of bounding boxes\n",
    "    order = np.argsort(score)\n",
    "\n",
    "def ann_pic(box_im, image_source,  labels):\n",
    "    detections = sv.Detections(xyxy=np.array(box_im))\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = cv2.cvtColor(image_source, cv2.COLOR_RGB2BGR)\n",
    "    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 groundinigdino  \n",
    "# Groundingdino(GD) autolabel to .xml\n",
    "# 程式過程: detect(GD) -> nms -> match template (del background) -> xml\n",
    "\n",
    "TEXT_PROMPT = 'defect' # prompt\n",
    "BOX_TRESHOLD = 0.2\n",
    "TEXT_TRESHOLD = 0.2\n",
    "folder_name = 'groundingdino_test'\n",
    "\n",
    "directory = 'C:\\\\Users\\\\yangu\\\\Desktop\\\\DMRV' # 指定資料夾下的圖片檔都會跑 grounding dino\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.jpg'):\n",
    "            try:\n",
    "                img_path = os.path.join(root, file)\n",
    "                image_source, image = load_image(img_path) \n",
    "                m_image = cv2.imread(img_path)\n",
    "                boxes, logits, phrases = predict(\n",
    "                    model=model,\n",
    "                    image=image,\n",
    "                    caption=TEXT_PROMPT,\n",
    "                    box_threshold=BOX_TRESHOLD,\n",
    "                    text_threshold=TEXT_TRESHOLD)\n",
    "                \n",
    "                # annotation_frame = annotate(image_source, boxes, logits, phrases)\n",
    "                bounding_boxes = bbox_convert(boxes, image_source)\n",
    "                nms_output = non_maximum_suppression_fast(bounding_boxes, overlapThresh=0.3)\n",
    "                tmp_image = m_image.copy()\n",
    "                for box in nms_output:\n",
    "                    box_int = box\n",
    "                    tmp = m_image[int(box_int[1]):int(box_int[3]), int(box_int[0]):int(box_int[2])]\n",
    "                    mask = np.ones_like(tmp) * 255\n",
    "                    tmp_image[int(box_int[1]):int(box_int[3]), int(box_int[0]):int(box_int[2])] = mask\n",
    "                objects = []\n",
    "                box_im, labels = [], []\n",
    "                for box in nms_output:\n",
    "                    box_int = box\n",
    "                    tmp = m_image[int(box_int[1]):int(box_int[3]), int(box_int[0]):int(box_int[2])]\n",
    "                    out_match = cv2.matchTemplate(tmp_image, tmp, cv2.TM_CCOEFF_NORMED)\n",
    "                    if len(np.where(out_match>0.97)[0]) == 0:\n",
    "                        save_box = [int(box_int[0]), int(box_int[2]), int(box_int[1]), int(box_int[3])]\n",
    "                        box_im.append([int(box_int[0]), int(box_int[1]), int(box_int[2]), int(box_int[3])])\n",
    "                        labels.append('defect')\n",
    "                        objects.append({\"name\": \"defect\", \"xmin\": str(save_box[0]), \"xmax\": str(save_box[1]), \"ymin\": str(save_box[2]), \"ymax\": str(save_box[3])})\n",
    "                if len(objects) > 0:\n",
    "                    filename_text = img_path.split('\\\\')[-1]\n",
    "                    folder_text = img_path.split('\\\\')[-2]\n",
    "                    save_path =  os.path.dirname(img_path)\n",
    "                    create_xml(objects, filename_text, folder_text,  str(m_image.shape[1]), str(m_image.shape[0]), save_path = save_path)\n",
    "                    # 畫圖\n",
    "                    # annotation_frame = ann_pic(box_im, image_source, labels)\n",
    "                    # a = 'D:\\\\yangu\\\\dataset\\\\groundingdino_test\\\\'+filename_text\n",
    "                    # cv2.imwrite(a, annotation_frame)\n",
    "\n",
    "            except Exception as err:\n",
    "                print(img_path)\n",
    "                print(err)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
